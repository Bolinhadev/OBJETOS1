<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Advanced Object and Person Detection</title>
  <style>
    video, canvas {
      position: absolute;
      top: 0;
      left: 0;
    }
    video {
      z-index: -1;
    }
  </style>
</head>
<body>

<!-- Video element to capture from the camera -->
<video id="video" width="640" height="480" autoplay></video>
<!-- Canvas to draw bounding boxes -->
<canvas id="canvas" width="640" height="480"></canvas>

<!-- Load TensorFlow.js -->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"> </script>
<!-- Load EfficientDet model for precise object detection -->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/efficientdet"></script>
<!-- Load MediaPipe Holistic for detecting persons and hands -->
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/holistic/holistic.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
<!-- Load YOLOv3 for detecting more complex objects -->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/yolo"></script>

<script>
  const videoElement = document.getElementById('video');
  const canvasElement = document.getElementById('canvas');
  const canvasCtx = canvasElement.getContext('2d');

  let efficientDetModel;
  let yoloModel;

  // Load the EfficientDet model
  async function loadEfficientDet() {
    efficientDetModel = await efficientdet.load({ version: 'lite0' });
  }

  // Load the YOLOv3 model
  async function loadYOLOv3() {
    yoloModel = await tf.loadGraphModel('https://tfhub.dev/tensorflow/yolo_v3/1/default/1', {fromTFHub: true});
  }

  // Initialize MediaPipe Holistic for detecting persons and hands
  const holistic = new Holistic({
    locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/holistic/${file}`
  });

  holistic.setOptions({
    modelComplexity: 1,
    smoothLandmarks: true,
    enableSegmentation: true,
    smoothSegmentation: true,
    refineFaceLandmarks: false,
    minDetectionConfidence: 0.5,
    minTrackingConfidence: 0.5
  });

  holistic.onResults(onResults);

  // Set up the camera stream
  const camera = new Camera(videoElement, {
    onFrame: async () => {
      await holistic.send({image: videoElement});
      await detectObjects();
    },
    width: 640,
    height: 480
  });
  camera.start();

  // Function to detect objects using both EfficientDet and YOLOv3
  async function detectObjects() {
    if (efficientDetModel) {
      const predictionsEfficientDet = await efficientDetModel.detect(videoElement);

      predictionsEfficientDet.forEach(prediction => {
        if (['phone', 'pencil', 'pen'].includes(prediction.class)) {
          const [x, y, width, height] = prediction.bbox;
          
          // Draw bounding box for detected objects by EfficientDet
          canvasCtx.beginPath();
          canvasCtx.rect(x, y, width, height);
          canvasCtx.lineWidth = 2;
          canvasCtx.strokeStyle = 'blue';
          canvasCtx.stroke();

          // Draw label
          canvasCtx.fillStyle = 'blue';
          canvasCtx.fillText(
            `${prediction.class} - ${Math.round(prediction.score * 100)}%`,
            x,
            y > 10 ? y - 5 : 10
          );
        }
      });
    }

    if (yoloModel) {
      const inputTensor = tf.browser.fromPixels(videoElement).expandDims(0);
      const predictionsYOLO = await yoloModel.predict(inputTensor);
      predictionsYOLO.forEach(prediction => {
        const [x, y, width, height] = prediction.bbox;

        // Draw bounding box for detected objects by YOLOv3
        canvasCtx.beginPath();
        canvasCtx.rect(x, y, width, height);
        canvasCtx.lineWidth = 2;
        canvasCtx.strokeStyle = 'green';
        canvasCtx.stroke();

        // Draw label
        canvasCtx.fillStyle = 'green';
        canvasCtx.fillText(
          `${prediction.class} - ${Math.round(prediction.score * 100)}%`,
          x,
          y > 10 ? y - 5 : 10
        );
      });
    }
  }

  // Function to handle results from MediaPipe (people and hands detection)
  function onResults(results) {
    // Draw segmentation mask for person
    canvasCtx.drawImage(results.segmentationMask, 0, 0, canvasElement.width, canvasElement.height);

    // If landmarks detected, draw them
    if (results.poseLandmarks) {
      drawLandmarks(results.poseLandmarks, 'red');
    }
    if (results.leftHandLandmarks) {
      drawLandmarks(results.leftHandLandmarks, 'green');
    }
    if (results.rightHandLandmarks) {
      drawLandmarks(results.rightHandLandmarks, 'blue');
    }
  }

  // Function to draw landmarks (MediaPipe landmarks for people and hands)
  function drawLandmarks(landmarks, color) {
    landmarks.forEach(landmark => {
      const x = landmark.x * canvasElement.width;
      const y = landmark.y * canvasElement.height;

      canvasCtx.beginPath();
      canvasCtx.arc(x, y, 5, 0, 2 * Math.PI);
      canvasCtx.fillStyle = color;
      canvasCtx.fill();
    });
  }

  // Load the models
  loadEfficientDet();
  loadYOLOv3();

</script>

</body>
</html>
