<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Multi-Model Object Detection</title>
  <style>
    video, canvas {
      position: absolute;
      top: 0;
      left: 0;
    }
    video {
      z-index: -1;
    }
  </style>
</head>
<body>

<!-- Video element to capture from the camera -->
<video id="video" width="640" height="480" autoplay></video>
<!-- Canvas to draw bounding boxes -->
<canvas id="canvas" width="640" height="480"></canvas>

<!-- Load TensorFlow.js -->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"> </script>
<!-- Load COCO-SSD model for detecting objects -->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"> </script>
<!-- Load YOLOv3 for detecting complex objects -->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/yolo"></script>
<!-- Load MediaPipe Holistic for detecting people and hands -->
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/holistic/holistic.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>

<script>
  const videoElement = document.getElementById('video');
  const canvasElement = document.getElementById('canvas');
  const canvasCtx = canvasElement.getContext('2d');

  let cocoSsdModel;
  let yoloModel;

  // Load COCO-SSD model for objects like pencil, pen, and phone
  cocoSsd.load().then(model => {
    cocoSsdModel = model;
  });

  // Load YOLOv3 model for advanced object detection
  tf.loadGraphModel('https://tfhub.dev/tensorflow/yolo_v3/1/default/1', {fromTFHub: true}).then(model => {
    yoloModel = model;
  });

  // Initialize MediaPipe Holistic for detecting people and hands
  const holistic = new Holistic({
    locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/holistic/${file}`
  });

  holistic.setOptions({
    modelComplexity: 1,
    smoothLandmarks: true,
    enableSegmentation: true,
    smoothSegmentation: true,
    refineFaceLandmarks: false,
    minDetectionConfidence: 0.5,
    minTrackingConfidence: 0.5
  });

  holistic.onResults(onResults);

  // Set up the camera stream
  const camera = new Camera(videoElement, {
    onFrame: async () => {
      await holistic.send({image: videoElement});
      await detectObjectsWithCocoSsd();
      await detectObjectsWithYolo();
    },
    width: 640,
    height: 480
  });
  camera.start();

  // Function to detect objects using COCO-SSD (pencil, phone, pen)
  async function detectObjectsWithCocoSsd() {
    if (cocoSsdModel) {
      const predictions = await cocoSsdModel.detect(videoElement);

      predictions.forEach(prediction => {
        if (['cell phone', 'pencil', 'pen'].includes(prediction.class)) {
          const [x, y, width, height] = prediction.bbox;
          
          // Draw bounding box for detected objects by COCO-SSD
          canvasCtx.beginPath();
          canvasCtx.rect(x, y, width, height);
          canvasCtx.lineWidth = 2;
          canvasCtx.strokeStyle = 'blue';
          canvasCtx.stroke();

          // Draw label
          canvasCtx.fillStyle = 'blue';
          canvasCtx.fillText(
            `${prediction.class} - ${Math.round(prediction.score * 100)}%`,
            x,
            y > 10 ? y - 5 : 10
          );
        }
      });
    }
  }

  // Function to detect objects using YOLOv3
  async function detectObjectsWithYolo() {
    if (yoloModel) {
      const inputTensor = tf.browser.fromPixels(videoElement).expandDims(0);
      const predictions = await yoloModel.executeAsync(inputTensor);

      // Get the bounding boxes and class labels from YOLOv3 results
      const boxes = predictions[0].arraySync();
      const scores = predictions[1].arraySync();
      const classes = predictions[2].arraySync();

      for (let i = 0; i < boxes.length; i++) {
        const [ymin, xmin, ymax, xmax] = boxes[i];
        const width = xmax - xmin;
        const height = ymax - ymin;

        if (scores[i] > 0.5) {
          const x = xmin * canvasElement.width;
          const y = ymin * canvasElement.height;

          // Draw bounding box for detected objects by YOLOv3
          canvasCtx.beginPath();
          canvasCtx.rect(x, y, width * canvasElement.width, height * canvasElement.height);
          canvasCtx.lineWidth = 2;
          canvasCtx.strokeStyle = 'green';
          canvasCtx.stroke();

          // Draw label
          canvasCtx.fillStyle = 'green';
          canvasCtx.fillText(
            `YOLO Object - ${Math.round(scores[i] * 100)}%`,
            x,
            y > 10 ? y - 5 : 10
          );
        }
      }

      tf.dispose([inputTensor, predictions]);
    }
  }

  // Function to handle results from MediaPipe (people and hands detection)
  function onResults(results) {
    // Draw segmentation mask for person
    canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
    canvasCtx.drawImage(results.segmentationMask, 0, 0, canvasElement.width, canvasElement.height);

    // If landmarks detected, draw them
    if (results.poseLandmarks) {
      drawLandmarks(results.poseLandmarks, 'red');
    }
    if (results.leftHandLandmarks) {
      drawLandmarks(results.leftHandLandmarks, 'green');
    }
    if (results.rightHandLandmarks) {
      drawLandmarks(results.rightHandLandmarks, 'blue');
    }
  }

  // Function to draw landmarks (MediaPipe landmarks for people and hands)
  function drawLandmarks(landmarks, color) {
    landmarks.forEach(landmark => {
      const x = landmark.x * canvasElement.width;
      const y = landmark.y * canvasElement.height;

      canvasCtx.beginPath();
      canvasCtx.arc(x, y, 5, 0, 2 * Math.PI);
      canvasCtx.fillStyle = color;
      canvasCtx.fill();
    });
  }

</script>

</body>
</html>
